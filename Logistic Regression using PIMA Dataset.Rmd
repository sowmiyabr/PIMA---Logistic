---
title: "PIMA DIabeties Dataset"
author: "IOA-Supriya"
date: "October 15, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages
# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats
# For example, here's several helpful packages to load in 


```{r}
library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function

install.packages("ggcorrplot")
install.packages("ROCR")

```





# Any results you write to the current directory are saved as output.
```{r}

# Load dataset
db = read.csv('/diabetes.csv', header=TRUE)
```


# Understand the structure of the dataset
```{r}
str(db)
```
# display first 6 rows of data
```{r}
head(db)
```

# display last 6 rows of data

```{r}
tail(db)
```


```{r}
summary(db)
```





# Create Age Category column
```{r}
db$Age_Cat <- ifelse(db$Age < 21, "<21", 
                   ifelse((db$Age>=21) & (db$Age<=25), "21-25", 
                   ifelse((db$Age>25) & (db$Age<=30), "25-30",
                   ifelse((db$Age>30) & (db$Age<=35), "30-35",
                   ifelse((db$Age>35) & (db$Age<=40), "35-40",
                   ifelse((db$Age>40) & (db$Age<=50), "40-50",
                   ifelse((db$Age>50) & (db$Age<=60), "50-60",">60")))))))

db$Age_Cat <- factor(db$Age_Cat, levels = c('<21','21-25','25-30','30-35','35-40','40-50','50-60','>60'))
table(db$Age_Cat)
```

# Histogram of Age
```{r}
library(ggplot2)

ggplot(aes(x = Age), data=db) +
        geom_histogram(binwidth=1, color='black', fill = "#F79420") +
        scale_x_continuous(limits=c(20,90), breaks=seq(20,90,5)) +
        xlab("Age") +
        ylab("No of people by age")

```





Most of the subjects are in between the ages 21 - 30






# Barplot by Age_Cat
```{r}
library(ggplot2)
ggplot(aes(x = Age_Cat), data = db) +
            geom_bar(fill='steelblue')

```





# box plot of Age_Cat vs BMI
```{r}
library(ggplot2)
ggplot(aes(x=Age_Cat, y = BMI), data = db) +
        geom_boxplot() +
        coord_cartesian(ylim = c(0,70))





by(db$BMI, db$Age_Cat, summary)

```

# Compute correlation matrix
```{r}
db_cor <- round(cor(db[1:8]),1)
db_cor





library(ggcorrplot)
ggcorrplot(db_cor)
```

No strong correlation observed between variables. So, no need to drop any of them for analysis

# Split dataset into train and test sets
```{r}
require(caTools)
set.seed(3)
sample = sample.split(db$Outcome, SplitRatio=0.75)
train = subset(db, sample==TRUE)
test = subset(db, sample==FALSE)

nrow(db)

nrow(train)

nrow(test)

```


# distribution of Age category in Train set
```{r}
table(train$Age_Cat)
```

# Structure of train set
```{r}
str(train)
```


# Baseline model
```{r}
table(db$Outcome)
```

# Baseline accuracy
```{r}
baseline <- round(500/nrow(db),2)
baseline

```

Do not select a model whose accuracy is lower than the baseline model. In this case, it is 0.65


# Fit model - using all independent variables
```{r}
AllVar <- glm(Outcome ~ ., data = train, family = binomial)
summary(AllVar)

```

# Let's predict outcome on Training dataset
```{r}

PredictTrain <- predict(AllVar, type = "response")
summary(PredictTrain)
```

# This computes the average prediction for each of the two outcomes

```{r}
tapply(PredictTrain, train$Outcome, mean)

```




#Build Confusion Matrix
#Confusion Matrix: Compares the actual outcomes with the predicted ones

<!-- Predicted = 0	Predicted = 1 -->
<!-- Actual = 0	True Negatives (TN)	False Positives (FP) -->
<!-- Actual = 1	False Negatives (FN)	True Positives (TP) -->
<!-- Sensitivity = TP / TP + FN (True Positive rate) -->
<!-- Specificity = TN / TN + FP (True Negative rate) -->

<!-- The model with a higher threshold has lower Sensitivity but higher Specificity. -->
<!-- The model with a lower threshold has higher Sensitivity but lower Specificity. -->

Thresholding:
The outcome of a logistic regression model is a probability.
We can do this using a threshold value t

<!-- If P(y=1) >= t, predict 1 -->
<!-- If P(y=1) < t, predict 0 -->
<!-- What value should be selected for t? -->

<!-- Often selected based on which errors are better -->

<!-- If t is large, predict P(y=1) rarely (when P(y=1) is large) -->
<!-- If t is small, predict P(y=0) rarely (when P(y=1) is small) -->

<!-- With no preference between errors, select t=0.5. Predicts the more likely outcome -->


# Build confusion matrix with a threshold value of 0.5
```{r}
threshold_0.5 <- table(train$Outcome, PredictTrain > 0.5)
threshold_0.5
```


# Accuracy
```{r}
accuracy_0.5 <- round(sum(diag(threshold_0.5))/sum(threshold_0.5),2)
sprintf("Accuracy is %s",accuracy_0.5)
```


# Mis-classification error rate
```{r}
MC_0.5 <- 1-accuracy_0.5
sprintf("Mis-classification error is %s",MC_0.5)

sensitivity0.5 <- round(118/(83+118),2)
specificity0.5 <- round(333/(333+42),2)
sprintf("Sensitivity at 0.5 threshold: %s", sensitivity0.5)
sprintf("Specificity at 0.5 threshold: %s", specificity0.5)

```
# Build confusion matrix with a threshold value of 0.7
```{r}

threshold_0.7 <- table(train$Outcome, PredictTrain > 0.7)
threshold_0.7

```

# Accuracy

```{r}
accuracy_0.7 <- round(sum(diag(threshold_0.7))/sum(threshold_0.7),2)
sprintf('Accuracy is %s', accuracy_0.7)
```

# Mis-classification error rate
```{r}
MC_0.7 <- 1-accuracy_0.7
sprintf("Mis-classification error is %s",MC_0.7)

sensitivity0.7 <- round(78/(123+78),2)
specificity0.7 <- round(359/(359+16),2)
sprintf("Sensitivity at 0.7 threshold: %s", sensitivity0.7)
sprintf("Specificity at 0.7 threshold: %s", specificity0.7)

```





# Build confusion matrix with a threshold value of 0.2

```{r}
threshold_0.2 <- table(train$Outcome, PredictTrain > 0.2)
threshold_0.2
```


# Accuracy
```{r}
accuracy_0.2 <- round(sum(diag(threshold_0.2))/sum(threshold_0.2),2)
sprintf("Accuracy is %s", accuracy_0.2)
```

# Mis-classification error rate
```{r}
MC_0.2 <- 1-accuracy_0.2
sprintf("Mis-classification error is %s",MC_0.2)

sensitivity0.2 <- round(180/(21+180),2)
specificity0.2 <- round(215/(215+160),2)
sprintf("Sensitivity at 0.2 threshold: %s",sensitivity0.2)
sprintf("Specificity at 0.2 threshold: %s",specificity0.2)
```

#ROC Curves (Receiver Operator Characteristic Curve)
#ROC Curve will help us decide as which threshold is best

#High threshold:

#High specificity
#Low sensitivity
#Low threshold:

#Low specificity
#High sensitivity



# Generate ROC Curves

```{r}
library(ROCR)

ROCRpred = prediction(PredictTrain, train$Outcome)
ROCRperf = performance(ROCRpred, "tpr", "fpr")
```


# Adding threshold labels
```{r}
plot(ROCRperf, colorize=TRUE, print.cutoffs.at = seq(0,1,0.1), text.adj = c(-0.2, 1.7))
abline(a=0, b=1)

auc_train <- round(as.numeric(performance(ROCRpred, "auc")@y.values),2)
legend(.8, .2, auc_train, title = "AUC", cex=1)
```

## Interpreting the model

**AUC (Area under the ROC curve)**: Absolute value of quality of prediction  

AUC = Maximum of 1 (Perfect prediction)  
AUC = minimum of 0.5 (just guessing)  
|            | Predicted class = 0        | Predicted class = 1        |
|------------|----------------------|----------------------|
| **Actual class = 0** | True Negatives (TN)  | False Positives (FP) |
| **Actual class = 1** | False Negatives (FN) | True Positives (TP)  | 

N = Number of obervations 
Overall accuracy = (TN + TP) / N  

- Sensitivity = TP/(TP + FN)  
- Specificity = TN/(TN + FP)  

Overall error rate = (FP + FN) / N  

- False Negative Error Rate = FN/(TP + FN)  
- False Positive Error Rate = FP/(TN + FP)  

***False positive error rate = 1 - specificity***

# Making predictions on test set
```{r}
PredictTest <- predict(AllVar, type = "response", newdata = test)
```



# Convert probabilities to values using the below

## Based on ROC curve above, selected a threshold of 0.5
```{r}
test_tab <- table(test$Outcome, PredictTest > 0.5)
test_tab

accuracy_test <- round(sum(diag(test_tab))/sum(test_tab),2)
sprintf("Accuracy on test set is %s", accuracy_test)

```

# Compute test set AUC
```{r}
ROCRPredTest = prediction(PredictTest, test$Outcome)
auc = round(as.numeric(performance(ROCRPredTest, "auc")@y.values),2)
auc

```

The AUC on the test set indicates that the predictive ability of the model is good
